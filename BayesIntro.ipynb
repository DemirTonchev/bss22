{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "rng_jesus = np.random.default_rng(seed=1337)\n",
    "\n",
    "params = {\n",
    "    # 'style':\n",
    "    # 'legend.fontsize': 'x-large',\n",
    "    # 'figure.figsize': (15, 5),\n",
    "    'axes.labelsize': 'x-large',\n",
    "    'axes.titlesize': 'x-large',\n",
    "    'xtick.labelsize': 'x-large',\n",
    "    'ytick.labelsize': 'x-large',\n",
    "    'legend.title_fontsize': 'x-large'}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-pastel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reasoning in a world with two states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_bern_bar(theta, p_theta, ax, **kwargs):\n",
    "    ax.bar(theta, p_theta, width=min(0.02, 1/len(theta)*0.3))\n",
    "    ax.set(xlim=(0-0.1, 1+0.1), ylim=(0, np.max(p_theta)*1.2),\n",
    "           **kwargs)\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "\n",
    "# show how data is generated\n",
    "p_success = 0.3\n",
    "bern_rv = st.bernoulli(p_success)\n",
    "bern_rv.rvs(random_state=rng_jesus, size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.repeat([1, 0], [1, 2])\n",
    "\n",
    "theta = np.array([0.3, 0.7])\n",
    "prior_theta = np.array([0.3, 0.7])\n",
    "# Create summary values of data\n",
    "z = data.sum()  # number of 1's in data\n",
    "N = len(data)  # number of flips in data\n",
    "# Compute the likelihood of the data for each value of theta.\n",
    "bern_likelihood = theta**z * (1 - theta)**(N - z)\n",
    "# Compute product of likelihood and prior (posterior)\n",
    "unstd_posterior = bern_likelihood * prior_theta\n",
    "posterior = unstd_posterior / unstd_posterior.sum()\n",
    "\n",
    "_, axes = plt.subplots(3, 1, figsize=(6, 12), sharex=True)\n",
    "axes[0].set(xlabel=r'$\\theta$')\n",
    "for ax, p, tlt, ylb in zip(axes,\n",
    "                           [prior_theta, bern_likelihood, posterior],\n",
    "                           ['Prior', 'Likelihood', 'Posterior'],\n",
    "                           [r'$P(\\theta)$', r'$P(\\theta|D)$', r'$P(\\theta|D)$']):\n",
    "    plot_bern_bar(theta, p, ax,\n",
    "                  title=tlt, ylabel=ylb)\n",
    "\n",
    "# %%\n",
    "# lets def a fun\n",
    "theta = np.array([0.3, 0.7])\n",
    "prior_theta = np.array([0.5, 0.5])\n",
    "\n",
    "\n",
    "def plot_prior_post_bern(theta, prior, data):\n",
    "\n",
    "    # Create summary values of data\n",
    "    z = data.sum()  # number of 1's in data\n",
    "    N = len(data)  # number of flips in data\n",
    "    # Compute the likelihood of the data for each value of theta.\n",
    "    bern_likelihood = theta**z * (1 - theta)**(N - z)\n",
    "    # Compute product of likelihood and prior (posterior)\n",
    "    unstd_posterior = bern_likelihood * prior\n",
    "    posterior = unstd_posterior / unstd_posterior.sum()\n",
    "\n",
    "    _, axes = plt.subplots(3, 1, figsize=(6, 12), sharex=True)\n",
    "    axes[0].set(xlabel=r'$\\theta$')\n",
    "    for ax, p, tlt, ylb in zip(axes,\n",
    "                               [prior_theta, bern_likelihood, posterior],\n",
    "                               ['Prior', 'Likelihood', 'Posterior'],\n",
    "                               [r'$P(\\theta)$', r'$P(\\theta|D)$', r'$P(\\theta|D)$']):\n",
    "        plot_bern_bar(theta, p, ax,\n",
    "                      title=tlt, ylabel=ylb)\n",
    "    return axes\n",
    "\n",
    "# achieve same result but find P(data) as denumerator\n",
    "\n",
    "\n",
    "# %%\n",
    "data = np.repeat([1, 0], [7, 3])\n",
    "\n",
    "# change eps\n",
    "eps = 0.1\n",
    "theta = np.arange(start=0+eps, stop=1, step=eps)\n",
    "prior_theta = np.repeat(1, len(theta))\n",
    "\n",
    "plot_prior_post_bern(theta, prior_theta, data)\n",
    "# %%\n",
    "# zadacha prior da e s 2 vyrha blizo do 0 i 1\n",
    "# ................\n",
    "\n",
    "\n",
    "# %% smooth version and conjugate priors\n",
    "\n",
    "def plt_beta(a, b, ax=None):\n",
    "    # generate fun support\n",
    "    x = np.linspace(start=0, stop=1, num=200)\n",
    "    # density of beta\n",
    "    p_x = st.beta.pdf(x, a, b)\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(x, p_x)\n",
    "    ax.set(xlim=(0-0.05, 1+0.05), ylim=(0, np.max(p_x[p_x < np.inf])*1.2))\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plt_conjugate_bernbeta(data, prior_a=1, prior_b=1):\n",
    "    # find opportunities for code refactoring\n",
    "    # Create summary values of data\n",
    "\n",
    "    z = data.sum()  # number of 1's in data\n",
    "    N = len(data)  # number of flips in data\n",
    "    post_a = prior_a+z\n",
    "    post_b = prior_b+N-z\n",
    "    # bernuli likelihood\n",
    "    theta = np.linspace(0, 1, 200)\n",
    "    bern_likelihood = theta**z * (1 - theta)**(N - z)\n",
    "\n",
    "    fig, (ax_prior, ax_lkhd, ax_post) = plt.subplots(3, 1, sharex=True)\n",
    "    fig.suptitle(f'success={z}, failure={N-z}', fontsize=16)\n",
    "    # prior\n",
    "    plt_beta(prior_a, prior_b, ax=ax_prior) \\\n",
    "        .set(title='Prior', ylabel=r'$P(\\theta)$')\n",
    "    # likelihood\n",
    "    ax_lkhd.plot(theta, bern_likelihood)\n",
    "    ax_lkhd.set(title='Likelihood', ylabel=r'$P(D|\\theta)$')\n",
    "    # posterior\n",
    "    plt_beta(post_a, post_b, ax=ax_post) \\\n",
    "        .set(title='Posterior', xlabel=r'$\\theta$', ylabel=r'$P(\\theta|D)$')\n",
    "\n",
    "\n",
    "data = np.repeat([1, 0], [2, 3])\n",
    "plt_conjugate_bernbeta(data, prior_a=1, prior_b=1)\n",
    "\n",
    "\n",
    "# %% same stuff with pymc\n",
    "\n",
    "data = np.repeat([1, 0], [2, 3])\n",
    "\n",
    "a_prior = 1\n",
    "b_prior = 1\n",
    "with pm.Model() as beta_bern_model:\n",
    "\n",
    "    theta = pm.Beta('theta', alpha=a_prior, beta=b_prior)\n",
    "    y = pm.Bernoulli('y', p=theta, observed=data)\n",
    "    idata = pm.sample(2000)\n",
    "\n",
    "# %% just for comparison\n",
    "z = data.sum()  # number of 1's in data\n",
    "N = len(data)\n",
    "pm_post_theta = idata.posterior.stack(sample=['chain', 'draw'])['theta'].data\n",
    "ax = plt_beta(a_prior+z, b_prior+N-z)\n",
    "ax.hist(pm_post_theta, bins=100, density=True)\n",
    "\n",
    "# %%\n",
    "# bounded beliefs\n",
    "\n",
    "with pm.Model() as beta_bern_model:\n",
    "\n",
    "    theta = pm.Bound('theta',\n",
    "                     pm.Beta.dist(alpha=a_prior, beta=b_prior),\n",
    "                     lower=0.5)\n",
    "    y = pm.Bernoulli('y', p=theta, observed=data)\n",
    "    idata = pm.sample(2000)\n",
    "\n",
    "pm_post_theta = idata.posterior.stack(sample=['chain', 'draw'])['theta'].data\n",
    "ax = plt_beta(a_prior+z, b_prior+N-z)\n",
    "ax.hist(pm_post_theta, bins=100, density=True)\n",
    "\n",
    "# %% generate data\n",
    "\n",
    "\n",
    "def generate_simple_reg(x, N=100, true_a=1, true_b=2, true_sd=1):\n",
    "    true_mu = true_a + true_b * x\n",
    "    return rng_jesus.normal(loc=true_mu, scale=true_sd, size=N)\n",
    "\n",
    "\n",
    "predictor = rng_jesus.normal(loc=0, scale=6, size=N)\n",
    "outcome = generate_simple_reg(predictor, true_sd=3)\n",
    "\n",
    "# plt.scatter(predictor, outcome)\n",
    "\n",
    "# %%\n",
    "\n",
    "prior_samples = 50\n",
    "with pm.Model() as model_1:\n",
    "    # prior on params\n",
    "    a = pm.Normal(\"a\", 0.0, 10.0)\n",
    "    b = pm.Normal(\"b\", 0.0, 10.0)\n",
    "    # deterministic function\n",
    "    mu = a + b * predictor\n",
    "    sigma = pm.Exponential(\"sigma\", 1.0)\n",
    "\n",
    "    pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=outcome)\n",
    "    idata = pm.sample_prior_predictive(\n",
    "        samples=prior_samples, random_seed=rng_jesus)\n",
    "    # idata.extend(pm.sample(1000, tune=2000, random_seed=rng_jesus))\n",
    "\n",
    "prior = idata.prior.stack(sample=(\"draw\", \"chain\")).copy()\n",
    "# %%\n",
    "\n",
    "x = np.linspace(-20, 20, 50)  # [:, None]\n",
    "# y = prior[\"a\"].data + prior[\"b\"].data * x\n",
    "\n",
    "# this can be vectorized\n",
    "a_samples = prior['a'].data\n",
    "b_samples = prior['b'].data\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "for i in range(prior_samples):\n",
    "    # this can be vectorized\n",
    "    ax.plot(x, a_samples[i] + b_samples[i] * x, \"b\", alpha=0.2)\n",
    "    ax.set_title(\"Prior predictive checks (weakly informative)\")\n",
    "\n",
    "# %% run the same with different priors :)\n",
    "# .......\n",
    "# %%\n",
    "# train the model\n",
    "\n",
    "with model_1:\n",
    "    idata.extend(pm.sample(tune=2000, random_seed=rng_jesus))\n",
    "\n",
    "# %%\n",
    "\n",
    "with model_1:\n",
    "    pm.sample_posterior_predictive(\n",
    "        idata, extend_inferencedata=True, random_seed=rng_jesus)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "d = pd.read_csv(\"data/Howell.csv\", sep=\";\", header=0)\n",
    "d = d[d.age >= 18]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bss22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f289f420827af3a0547bdda076932220cd02828c023b09934e02ac9d495860"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
